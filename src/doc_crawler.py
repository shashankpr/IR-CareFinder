import requests
from bs4 import BeautifulSoup
import re

#one instance for one hospital
class doc_craw:

    def __init__(self,ori_url,page_limit=5,thres=0.8):
        self.ori_url=ori_url
        self.page_limit=page_limit
        self.doc_links={}  #doctor name: doctor url
        self.thres=thres #judge whether it is a general search or a doctor name search


    #the input url has to be the search result list
    # search for all M.D. in this website
    # and check whether there is a list containing the MDs
    # if so, count the total number of items in the list
    def check_list(self,url, page_count=0):
        if page_count>=self.page_limit:
            return

        r = requests.get(url)
        soup = BeautifulSoup(r.text, 'lxml')

        #1.add all the doctor name's link in this page to the list
        l_soup = soup.find_all(text=re.compile("(\W+)M\.?D\.?(\W*)"))
        for item in l_soup:
            print(item)
            try:
                tmp_link=item.find_parent("a")['href']
            except:
                tmp_link=''
            self.doc_links[item.strip()]=tmp_link

        #check whether it is a general search in the first page
        if page_count==0:
            block = l_soup[0]
            # print(block.find_next_sibling('a'))
            for i in range(15):  # 9
                print("-------------------------")
                block = block.parent
                tmp = str(block.find_next_sibling('div'))
                if tmp is not None and l_soup[1] in tmp:
                    break
            print(block.name, block.get('class', ''))

            next_block = block.find_next_sibling('div')
            next_count = 0
            while next_block is not None and next_block.get("class", '') == block.get('class', ''):
                print(next_block.get('class'))
                next_count += 1
                next_block = next_block.find_next_sibling('div')
                # if next_block is None:
                #     break

            print("next->front")

            prev_block = block.find_previous_sibling('div')
            prev_count = 0
            while prev_block is not None and prev_block.get("class", '') == block.get('class', ''):
                print(prev_block.get('class'))
                prev_count += 1
                prev_block = prev_block.find_previous_sibling('div')

            count = next_count + prev_count + 1
            print("count", count)

            if len(l_soup) / count <=self.thres:
                #this page is generated by general search
                return


        ####################next page

        #find the url to the next page
        ch_page = soup.find_all(text=["next", "Next","Â»"])
        print(len(ch_page)) #there might be multiple a tag with "next"
        if len(ch_page)==0:
            return
        p_a_href=''
        for item in ch_page:
            try:
                p_a_href = item.find_parent("a")['href']
            except:
                continue

            valid = re.compile("\S+" + "page")
            if valid.match(p_a_href):
                print(p_a_href)
                p_a_href=self.complete_url(p_a_href)
                if "http" not in p_a_href:
                    #incomlete url, need to be changed
                    base_url=re.search(re.compile("^\S+/"),url).group()
                    p_a_href=base_url[:-1]+p_a_href

                print("success")
        if p_a_href=='':
            return
        self.check_list(p_a_href,page_count+1)

    def complete_url(self,url):
        if "http" not in url:
            # incomlete url, need to be changed
            base_url = self.ori_url[:-1] if self.ori_url[-1]=='/' else self.ori_url
            return base_url+url
        else:
            return url

    #from main page to the doctors page
    def get_doctors_page(self):
        r=requests.get(self.ori_url)
        soup=BeautifulSoup(r.text,"lxml")
        a_result=soup.find_all(text=["find a doctor","doctor","FIND A DOCTOR"])
        print(len(a_result))

        for item in a_result:
            try:
                doc_ref=item.find_parent("a")['href']
                self.check_list(self.complete_url(doc_ref))
            except:
                continue
        # print(self.complete_url(doc_ref))
    
    #look for search box, and check for its search results.
    def form_match(self,url,search_term):

        r=requests.get(url)
        soup=BeautifulSoup(r.text,"lxml")
        forms=soup.find_all("input")
        for item in forms:
            tmp_pre=item.find_parent("form").get("action",'')
            tmp_mea=item.get('name','')
            if tmp_mea==['s','q','search']:
                # rr=requests.get(self.complete_url(tmp_pre)+"?"+tmp_mea+"="+search_term)
                # r_soup=BeautifulSoup(rr.text,'lxml')
                self.check_list(self.complete_url(tmp_pre)+"?"+tmp_mea+"="+search_term)
                break #currently only check the first successful search
            else:
                continue
            # print(tmp_pre,item.get("name",''))
